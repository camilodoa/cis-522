{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fancy-hierarchy",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "presidential-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.8.1-cp39-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 119.6 MB 6.2 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.20.2-cp39-cp39-macosx_10_9_x86_64.whl (16.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.1 MB 29.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, numpy, torch\n",
      "Successfully installed numpy-1.20.2 torch-1.8.1 typing-extensions-3.7.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (1.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.2-cp39-cp39-macosx_10_11_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 47.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (1.20.2)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.44.tar.gz (862 kB)\n",
      "\u001b[K     |████████████████████████████████| 862 kB 32.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp39-cp39-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 25.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (2.15.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 36.9 MB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy 'setup.py install' for sacremoses, since package 'wheel' is not installed.\n",
      "Installing collected packages: tqdm, regex, joblib, click, tokenizers, sacremoses, filelock, transformers\n",
      "    Running setup.py install for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed click-7.1.2 filelock-3.0.12 joblib-1.0.1 regex-2021.4.4 sacremoses-0.0.44 tokenizers-0.10.2 tqdm-4.60.0 transformers-4.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.2.3-cp39-cp39-macosx_10_9_x86_64.whl (10.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.7 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (5.0.5)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (7.22.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (5.1.3)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (5.5.3)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tornado>=4.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: appnope in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (54.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: decorator in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (5.0.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt_toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.18)\n",
      "Requirement already satisfied: pygments in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: backcall in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: ipython_genutils in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter_core in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from prompt_toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.0.3)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.4)\n",
      "Requirement already satisfied: prometheus_client in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab_pygments in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: bleach in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: testpath in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: async-generator in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.9)\n",
      "Requirement already satisfied: webencodings in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-7.6.3 jupyterlab-widgets-1.0.0 widgetsnbextension-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (4.60.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install ipywidgets\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-viking",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broad-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from ipywidgets import IntProgress\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-report",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We'll be using the _Offensive Language Identification Dataset (OLID)_ dataset to specialize BERT in hate speech detection. You can find the dataset [here](https://scholar.harvard.edu/malmasi/olid). The following code will download it into the current dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smaller-modeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'OLID'...\n",
      "remote: Enumerating objects: 35, done.\u001b[K\n",
      "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 35 (delta 18), reused 28 (delta 11), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (35/35), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/idontflow/OLID.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-document",
   "metadata": {},
   "source": [
    "### Examine our data\n",
    "\n",
    "OLID (2019) is comprised of a number of datasets. Let's break them down now. olid-training-v1.tsv has ~13,000 annotated tweets and 3 subtask labels. Then we have our test sets for each subtask: testset-levela.tsv, testset-levelb.tsv, testset-levelc.tsv.\n",
    "\n",
    "subtask_a is a categorization on whether the tweet is offensive. Here are the categories:\n",
    "\n",
    "- (NOT) Not Offensive - This post does not contain offense or profanity.\n",
    "- (OFF) Offensive - This post contains offensive language or a targeted (veiled or direct) offense\n",
    "\n",
    "If a tweet was labeled as offensive, then it can have a value for subtask B. Here are the categories for subtask_b:\n",
    "\n",
    "- (TIN) Targeted Insult and Threats - A post containing an insult or threat to an individual, a group, or others (see categories in sub-task C).\n",
    "- (UNT) Untargeted - A post containing non-targeted profanity and swearing.\n",
    "\n",
    "If a tweet was marked as offensive and targeted, then it can have the following categories for subtask_c:\n",
    "\n",
    "- (IND) Individual - The target of the offensive post is an individual: a famous person, a named individual or an unnamed person interacting in the conversation.\n",
    "- (GRP) Group - The target of the offensive post is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or something else.\n",
    "- (OTH) Other – The target of the offensive post does not belong to any of the previous two categories (e.g., an organization, a situation, an event, or an issue)\n",
    "\n",
    "Of course, if a tweet isn't offensive, it won't have values for subtask b and c. Similarly, if the tweet is untargeted, it won't have a value for subtask c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-banana",
   "metadata": {},
   "source": [
    "Now, let's take a look at the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occasional-mountain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./OLID/olid-training-v1.0.tsv', sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-jesus",
   "metadata": {},
   "source": [
    "We want to make sure we can run this code on CUDA, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "frank-thursday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-domestic",
   "metadata": {},
   "source": [
    "Let's start by making a classifier for generally offensive tweets (subtask_a). We'll make a pytorch model that extends roBERTa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-columbia",
   "metadata": {},
   "source": [
    "### subtask_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-camping",
   "metadata": {},
   "source": [
    "We'll be using roBERTa's tokenizer to encode our text. Behind the scene, it's doing byte-level byte pair encoding (BPE). Let's also define some constants here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", truncation=True)\n",
    "batch_size = 32\n",
    "train_size = 0.8 \n",
    "max_len = 280\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-accreditation",
   "metadata": {},
   "source": [
    "Let's clean up our dataframe by converting \"OFF\" to 1 and \"NOT\" to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "incorrect-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...         1   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...         1   \n",
       "2  16820  Amazon is investigating Chinese employees who ...         0   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...         1   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...         0   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data[\"subtask_a\"] == \"OFF\", \"subtask_a\"] = 1\n",
    "data.loc[data[\"subtask_a\"] == \"NOT\", \"subtask_a\"] = 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-savannah",
   "metadata": {},
   "source": [
    "Let's define a Data class for this task. We'll extend pytorch's Dataset class here so we can initialize DataLoaders with it down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "alternative-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataA(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        # Our data\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # X and Y\n",
    "        self.text = self.data.tweet\n",
    "        self.targets = self.data.subtask_a\n",
    "    \n",
    "    # Some class methods we have to override:\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Extract the text at index\n",
    "        text = str(self.text[index])\n",
    "        text = \"\".join(text.split())\n",
    "        \n",
    "        # Create a dictionary that contains the encoded sequence\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        \n",
    "        # Extract the values we care about from inputs\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-armenia",
   "metadata": {},
   "source": [
    "Let's make our train/test sets!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "norwegian-backing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10592, 5)\n",
      "Test shape: (2648, 5)\n"
     ]
    }
   ],
   "source": [
    "# Sample from our dataset \n",
    "train_data = data.sample(frac=train_size, random_state=0)\n",
    "# Remove the train data we sampled in the last step to get our training data\n",
    "test_data = data.drop(train_data.index).reset_index(drop=True)\n",
    "# Clean up train data by resetting the indexes\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Our train and test data shape\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "\n",
    "# Our data\n",
    "train_set = DataA(train_data, tokenizer, max_len)\n",
    "test_set = DataA(test_data, tokenizer, max_len)\n",
    "\n",
    "# Our dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-diamond",
   "metadata": {},
   "source": [
    "Our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "norman-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "\n",
    "        # Layer 1 is Roberta\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # First hidden layer for our classifier\n",
    "        self.l2 = torch.nn.Linear(768, 768)\n",
    "        # ReLU\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        # Output layer\n",
    "        self.l3 = torch.nn.Linear(768, 2)\n",
    "        # Softmax\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Roberta inputs\n",
    "        x = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        x = self.l2(x[0][:, 0])\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1abcf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred, target):\n",
    "    n_correct = (pred==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-religion",
   "metadata": {},
   "source": [
    "Let's train this jawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a049ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = ModelA()\n",
    "modelA.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer_fn = torch.optim.Adam(params =  modelA.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:28, 88.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6616766452789307\n",
      "Training Accuracy per 5000 steps: 84.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "331it [7:46:47, 84.61s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 0: 66.32364048338368\n",
      "Training loss epoch: 0.6477830762949595\n",
      "Training accuracy epoch: 66.32364048338368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:23, 83.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.677889347076416\n",
      "Training Accuracy per 5000 steps: 62.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [3:33:01, 109.57s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    curr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    modelA.train()\n",
    "    # Loop over batch data\n",
    "    # btw tqdm is a nice loop progress visualizer\n",
    "    for step, example in tqdm(enumerate(train_loader, 0)):\n",
    "        # Convert example from batch into input for Roberta\n",
    "        ids = example['ids'].to(device, dtype = torch.long)\n",
    "        mask = example['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = example['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = example['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = modelA(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # Add loss to current epoch loss tracker\n",
    "        curr_loss += loss.item()\n",
    "        \n",
    "        # Add to our counters\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        # Calculate backward pass\n",
    "        optimizer_fn.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer_fn.step()\n",
    "        if step % 5000 == 0:\n",
    "            loss_step = curr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "        \n",
    "    print(f'The total accuracy for epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = curr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {epoch_accu}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990da65",
   "metadata": {},
   "source": [
    "Now, let's save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61450c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelA, 'hate_speech_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85880c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
