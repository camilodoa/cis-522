{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fancy-hierarchy",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install ipywidgets\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-viking",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from ipywidgets import IntProgress\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-report",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We'll be using the _Offensive Language Identification Dataset (OLID)_ dataset to specialize BERT in hate speech detection. You can find the dataset [here](https://scholar.harvard.edu/malmasi/olid). The following code will download it into the current dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/idontflow/OLID.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-document",
   "metadata": {},
   "source": [
    "### Examine our data\n",
    "\n",
    "OLID (2019) is comprised of a number of datasets. Let's break them down now. olid-training-v1.tsv has ~13,000 annotated tweets and 3 subtask labels. Then we have our test sets for each subtask: testset-levela.tsv, testset-levelb.tsv, testset-levelc.tsv.\n",
    "\n",
    "subtask_a is a categorization on whether the tweet is offensive. Here are the categories:\n",
    "\n",
    "- (NOT) Not Offensive - This post does not contain offense or profanity.\n",
    "- (OFF) Offensive - This post contains offensive language or a targeted (veiled or direct) offense\n",
    "\n",
    "If a tweet was labeled as offensive, then it can have a value for subtask B. Here are the categories for subtask_b:\n",
    "\n",
    "- (TIN) Targeted Insult and Threats - A post containing an insult or threat to an individual, a group, or others (see categories in sub-task C).\n",
    "- (UNT) Untargeted - A post containing non-targeted profanity and swearing.\n",
    "\n",
    "If a tweet was marked as offensive and targeted, then it can have the following categories for subtask_c:\n",
    "\n",
    "- (IND) Individual - The target of the offensive post is an individual: a famous person, a named individual or an unnamed person interacting in the conversation.\n",
    "- (GRP) Group - The target of the offensive post is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or something else.\n",
    "- (OTH) Other â€“ The target of the offensive post does not belong to any of the previous two categories (e.g., an organization, a situation, an event, or an issue)\n",
    "\n",
    "Of course, if a tweet isn't offensive, it won't have values for subtask b and c. Similarly, if the tweet is untargeted, it won't have a value for subtask c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-banana",
   "metadata": {},
   "source": [
    "Now, let's take a look at the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-mountain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./OLID/olid-training-v1.0.tsv', sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-jesus",
   "metadata": {},
   "source": [
    "We want to make sure we can run this code on CUDA, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-domestic",
   "metadata": {},
   "source": [
    "Let's start by making a classifier for generally offensive tweets (subtask_a). We'll make a pytorch model that extends roBERTa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-columbia",
   "metadata": {},
   "source": [
    "### subtask_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-camping",
   "metadata": {},
   "source": [
    "We'll be using roBERTa's tokenizer to encode our text. Behind the scene, it's doing byte-level byte pair encoding (BPE). Let's also define some constants here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", truncation=True)\n",
    "batch_size = 32\n",
    "train_size = 0.8 \n",
    "max_len = 140\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-accreditation",
   "metadata": {},
   "source": [
    "Let's clean up our dataframe by converting \"OFF\" to 1 and \"NOT\" to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"subtask_a\"] == \"OFF\", \"subtask_a\"] = 1\n",
    "data.loc[data[\"subtask_a\"] == \"NOT\", \"subtask_a\"] = 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-savannah",
   "metadata": {},
   "source": [
    "Let's define a Data class for this task. We'll extend pytorch's Dataset class here so we can initialize DataLoaders with it down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataA(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        # Our data\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # X and Y\n",
    "        self.text = self.data.tweet\n",
    "        self.targets = self.data.subtask_a\n",
    "    \n",
    "    # Some class methods we have to override:\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Extract the text at index\n",
    "        text = str(self.text[index])\n",
    "        text = \"\".join(text.split())\n",
    "        \n",
    "        # Create a dictionary that contains the encoded sequence\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        # Extract the values we care about from inputs\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-armenia",
   "metadata": {},
   "source": [
    "Let's make our train/test sets!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from our dataset \n",
    "train_data = data.sample(frac=train_size, random_state=0)\n",
    "# Remove the train data we sampled in the last step to get our training data\n",
    "test_data = data.drop(train_data.index).reset_index(drop=True)\n",
    "# Clean up train data by resetting the indexes\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Our train and test data shape\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "\n",
    "# Our data\n",
    "train_set = DataA(train_data, tokenizer, max_len)\n",
    "test_set = DataA(test_data, tokenizer, max_len)\n",
    "\n",
    "# Our dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-diamond",
   "metadata": {},
   "source": [
    "Our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "\n",
    "        # Layer 1 is Roberta\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # First hidden layer for our classifier\n",
    "        self.l2 = torch.nn.Linear(768, 768)\n",
    "        # ReLU\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        # Output layer\n",
    "        self.l3 = torch.nn.Linear(768, 2)\n",
    "        # Softmax\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Roberta inputs\n",
    "        x = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        x = self.l2(x[0][:, 0])\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-religion",
   "metadata": {},
   "source": [
    "Let's train this jawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = ModelA()\n",
    "modelA.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer_fn = torch.optim.Adam(params =  modelA.parameters())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    curr_loss = 0\n",
    "    modelA.train()\n",
    "    # Loop over batch data\n",
    "    # btw tqdm is a nice loop progress visualizer\n",
    "    for step, example in tqdm(enumerate(train_loader, 0)):\n",
    "        # Convert example from batch into input for Roberta\n",
    "        ids = example['ids'].to(device, dtype = torch.long)\n",
    "        mask = example['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = example['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = example['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = modelA(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # Add loss to current epoch loss tracker\n",
    "        curr_loss += loss.item()\n",
    "        \n",
    "        # Calculate backward pass\n",
    "        optimizer_fn.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "    print(\"Total loss for epoch {epoch} was {loss}\".format(epoch, curr_loss))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-canal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
