{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fancy-hierarchy",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "presidential-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from torch) (1.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (1.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (4.5.0)\n",
      "Requirement already satisfied: packaging in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (2.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (1.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (7.6.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (7.22.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (1.0.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (5.0.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipywidgets) (5.5.3)\n",
      "Requirement already satisfied: appnope in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (54.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: decorator in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (5.0.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt_toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.18)\n",
      "Requirement already satisfied: pygments in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: backcall in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: ipython_genutils in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter_core in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.3.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from prompt_toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.0.3)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.4)\n",
      "Requirement already satisfied: prometheus_client in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab_pygments in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: bleach in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: testpath in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: async-generator in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.9)\n",
      "Requirement already satisfied: webencodings in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/3.0.13/libexec/lib/python3.9/site-packages (4.60.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install transformers\n",
    "%pip install pandas\n",
    "%pip install ipywidgets\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-viking",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broad-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from ipywidgets import IntProgress\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-report",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We'll be using the _Offensive Language Identification Dataset (OLID)_ dataset to specialize BERT in hate speech detection. You can find the dataset [here](https://scholar.harvard.edu/malmasi/olid). The following code will download it into the current dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smaller-modeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'OLID' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/idontflow/OLID.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-document",
   "metadata": {},
   "source": [
    "### Examine our data\n",
    "\n",
    "OLID (2019) is comprised of a number of datasets. Let's break them down now. olid-training-v1.tsv has ~13,000 annotated tweets and 3 subtask labels. Then we have our test sets for each subtask: testset-levela.tsv, testset-levelb.tsv, testset-levelc.tsv.\n",
    "\n",
    "subtask_a is a categorization on whether the tweet is offensive. Here are the categories:\n",
    "\n",
    "- (NOT) Not Offensive - This post does not contain offense or profanity.\n",
    "- (OFF) Offensive - This post contains offensive language or a targeted (veiled or direct) offense\n",
    "\n",
    "If a tweet was labeled as offensive, then it can have a value for subtask B. Here are the categories for subtask_b:\n",
    "\n",
    "- (TIN) Targeted Insult and Threats - A post containing an insult or threat to an individual, a group, or others (see categories in sub-task C).\n",
    "- (UNT) Untargeted - A post containing non-targeted profanity and swearing.\n",
    "\n",
    "If a tweet was marked as offensive and targeted, then it can have the following categories for subtask_c:\n",
    "\n",
    "- (IND) Individual - The target of the offensive post is an individual: a famous person, a named individual or an unnamed person interacting in the conversation.\n",
    "- (GRP) Group - The target of the offensive post is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or something else.\n",
    "- (OTH) Other – The target of the offensive post does not belong to any of the previous two categories (e.g., an organization, a situation, an event, or an issue)\n",
    "\n",
    "Of course, if a tweet isn't offensive, it won't have values for subtask b and c. Similarly, if the tweet is untargeted, it won't have a value for subtask c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-banana",
   "metadata": {},
   "source": [
    "Now, let's take a look at the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "occasional-mountain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./OLID/olid-training-v1.0.tsv', sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-jesus",
   "metadata": {},
   "source": [
    "We want to make sure we can run this code on CUDA, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "frank-thursday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-domestic",
   "metadata": {},
   "source": [
    "Let's start by making a classifier for generally offensive tweets (subtask_a). We'll make a pytorch model that extends roBERTa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-columbia",
   "metadata": {},
   "source": [
    "### subtask_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-camping",
   "metadata": {},
   "source": [
    "We'll be using roBERTa's tokenizer to encode our text. Behind the scene, it's doing byte-level byte pair encoding (BPE). Let's also define some constants here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nuclear-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", truncation=True)\n",
    "batch_size = 32\n",
    "train_size = 0.8 \n",
    "max_len = 280\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-accreditation",
   "metadata": {},
   "source": [
    "Let's clean up our dataframe by specifying that we're looking for targetted hate speech. Text should be labeled as hate speech if it's offensive and targetted towards an individual or a group of people. We don't care if people are just using profanity online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "incorrect-sauce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...         1   \n",
       "2  16820  Amazon is investigating Chinese employees who ...         0   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...         0   \n",
       "5  97670                  @USER Liberals are all Kookoo !!!         1   \n",
       "7  52415  @USER was literally just talking about this lo...         1   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "4       NaN       NaN  \n",
       "5       TIN       OTH  \n",
       "7       TIN       GRP  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[(data[\"subtask_a\"] == \"OFF\") & (data[\"subtask_b\"] == \"TIN\"), \"subtask_a\"] = 1\n",
    "data.loc[data[\"subtask_a\"] == \"NOT\", \"subtask_a\"] = 0\n",
    "data = data[(data[\"subtask_a\"] == 0) |  (data[\"subtask_a\"] == 1)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-savannah",
   "metadata": {},
   "source": [
    "Let's define a Data class for this task. We'll extend pytorch's Dataset class here so we can initialize DataLoaders with it down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "alternative-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataA(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        # Our data\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # X and Y\n",
    "        self.text = self.data.tweet\n",
    "        self.targets = self.data.subtask_a\n",
    "    \n",
    "    # Some class methods we have to override:\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Extract the text at index\n",
    "        text = str(self.text[index])\n",
    "        text = \"\".join(text.split())\n",
    "        \n",
    "        # Create a dictionary that contains the encoded sequence\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        \n",
    "        # Extract the values we care about from inputs\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-armenia",
   "metadata": {},
   "source": [
    "Let's make our train/test sets!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "norwegian-backing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10173, 5)\n",
      "Test shape: (2543, 5)\n"
     ]
    }
   ],
   "source": [
    "# Sample from our dataset \n",
    "train_data = data.sample(frac=train_size, random_state=0)\n",
    "# Remove the train data we sampled in the last step to get our training data\n",
    "test_data = data.drop(train_data.index).reset_index(drop=True)\n",
    "# Clean up train data by resetting the indexes\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Our train and test data shape\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "\n",
    "# Our data\n",
    "train_set = DataA(train_data, tokenizer, max_len)\n",
    "test_set = DataA(test_data, tokenizer, max_len)\n",
    "\n",
    "# Our dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-diamond",
   "metadata": {},
   "source": [
    "Our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "norman-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "\n",
    "        # Layer 1 is Roberta\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # First hidden layer for our classifier\n",
    "        self.l2 = torch.nn.Linear(768, 768)\n",
    "        # ReLU\n",
    "        self.relu = torch.nn.ReLU() \n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        # Output layer\n",
    "        self.l3 = torch.nn.Linear(768, 2)\n",
    "        # Softmax\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Roberta inputs\n",
    "        x = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        x = self.l2(x[0][:, 0])\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc570ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred, target):\n",
    "    n_correct = (pred==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-religion",
   "metadata": {},
   "source": [
    "Let's train this jawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "346e3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = ModelA()\n",
    "modelA.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer_fn = torch.optim.Adam(params =  modelA.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "representative-bulgaria",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:37, 97.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.7269188165664673\n",
      "Training Accuracy per 5000 steps: 25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [8:01:02, 90.76s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 0: 69.38956060159245\n",
      "Training loss epoch: 0.6185846966954897\n",
      "Training accuracy epoch: 69.38956060159245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:20, 80.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6257608532905579\n",
      "Training Accuracy per 5000 steps: 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:11:20, 81.38s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 1: 69.59598938366264\n",
      "Training loss epoch: 0.6172864417422492\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:22, 82.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.5945117473602295\n",
      "Training Accuracy per 5000 steps: 71.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:41:03, 86.99s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 2: 69.59598938366264\n",
      "Training loss epoch: 0.617252786485654\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:22, 82.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6257616281509399\n",
      "Training Accuracy per 5000 steps: 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:27:17, 84.39s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 3: 69.59598938366264\n",
      "Training loss epoch: 0.6173239481524102\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:23, 83.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6257616877555847\n",
      "Training Accuracy per 5000 steps: 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:14:03, 81.90s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 4: 69.59598938366264\n",
      "Training loss epoch: 0.6172731177611921\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:21, 81.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.5945116877555847\n",
      "Training Accuracy per 5000 steps: 71.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:25:59, 84.15s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 5: 69.59598938366264\n",
      "Training loss epoch: 0.6172629470157923\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:23, 83.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.5632616877555847\n",
      "Training Accuracy per 5000 steps: 75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:21:46, 83.35s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 6: 69.59598938366264\n",
      "Training loss epoch: 0.6172731153245242\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:23, 83.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.7195115685462952\n",
      "Training Accuracy per 5000 steps: 59.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:21:47, 83.36s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 7: 69.59598938366264\n",
      "Training loss epoch: 0.6173036139716143\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:23, 83.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.5320117473602295\n",
      "Training Accuracy per 5000 steps: 78.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:34:08, 85.69s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 8: 69.59598938366264\n",
      "Training loss epoch: 0.6173137813431662\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:26, 86.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6882616281509399\n",
      "Training Accuracy per 5000 steps: 62.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [7:25:19, 84.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy for epoch 9: 69.59598938366264\n",
      "Training loss epoch: 0.6172934485681402\n",
      "Training accuracy epoch: 69.59598938366264\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    curr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    modelA.train()\n",
    "    # Loop over batch data\n",
    "    # btw tqdm is a nice loop progress visualizer\n",
    "    for step, example in tqdm(enumerate(train_loader, 0)):\n",
    "        # Convert example from batch into input for Roberta\n",
    "        ids = example['ids'].to(device, dtype = torch.long)\n",
    "        mask = example['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = example['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = example['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = modelA(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        # Add loss to current epoch loss tracker\n",
    "        curr_loss += loss.item()\n",
    "        \n",
    "        # Add to our counters\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        # Calculate backward pass\n",
    "        optimizer_fn.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer_fn.step()\n",
    "        if step % 5000 == 0:\n",
    "            loss_step = curr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "        \n",
    "    print(f'The total accuracy for epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = curr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {epoch_accu}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f8c93",
   "metadata": {},
   "source": [
    "Now, let's save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c220fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelA, 'hate_speech_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeda0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
